##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Mike Morgan
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline GWAS
===========================

:Author: Mike Morgan
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_gwas.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* gcta >= 1.25.0
* plink >= 1.9
* bolt-lmm >= 2.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import PipelineGWAS as gwas

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh

# ---------------------------------------------------
# load the UKBiobank phenotype data into an SQLite DB
# use this as the main accessor of phenotype data
# for the report and non-genetic analyses
@follows(mkdir("phenotypes.dir"),
         mkdir("%s" % PARAMS['plots_dir']))
@transform("%s/*.tab" % PARAMS['data_dir'],
           regex("%s/(.+).tab" % PARAMS['data_dir']),
           r"phenotypes.dir/\1.tsv")
def formatPhenotypeData(infiles, outfile):
    '''
    Use the UKBiobank encoding dictionary/R script to
    set the factor levels for phenotype data.
    Output is in plink covariate file format
    '''

    pheno_file = infiles

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2pheno.py
    --task=plink_format
    --id-variable=%(data_id_var)s
    --log=%(outfile)s.log
    %(pheno_file)s
    > %(outfile)s
    '''

    P.run()

@follows(formatPhenotypeData)
@transform(formatPhenotypeData,
           suffix(".tsv"),
           ".load")
def loadPhenotypes(infile, outfile):
    '''
    load all phenotype data in to an SQLite DB
    '''

    P.load(infile, outfile)


@follows(loadPhenotypes)
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"phenotypes.dir/\1_British.tsv")
def selectBritish(infile, outfile):
    '''
    Select only those individuals with a white British
    ethnicity
    '''

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2pheno.py
    --task=select_ethnicity
    --ethnicity-id=%(format_ethnicity_var)s
    --ethnicity-label=%(format_ethnicity)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()

@follows(selectBritish)
@transform(selectBritish,
           suffix("_British.tsv"),
           ".keep")
def makeKeepFile(infile, outfile):
    '''
    make a samples.keep file for filtering
    on individuals
    '''

    statement = '''
    cat %(infile)s | awk '{if(NR > 1) {printf("%%s\\t%%s\\n", $1, $2)}}'
    > %(outfile)s
    '''

    P.run()


@follows(loadPhenotypes,
         selectBritish)
@transform(selectBritish,
           regex("phenotypes.dir/(.+)_British.tsv"),
           r"phenotypes.dir/\1.pheno")
def dichotimisePhenotype(infile, outfile):
    '''
    Dichotomise a phenotype for association testing
    '''

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2pheno.py
    --task=dichotimise_phenotype
    --pheno-id=%(data_dichot_var)s
    --reference-variable=%(data_reference_value)s
    --missing-var-label=%(data_missing_label)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(loadPhenotypes,
         mkdir("plots.dir"))
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"plots.dir/1\_phenotype.png")
def plotPhenotypeData(infile, outfile):
    '''
    Generare plots of phenotype distributions
    for CGATReport document
    '''

    pass


@follows(loadPhenotypes,
         plotPhenotypeData)
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"plots.dir/\1_map_%s.png" % PARAMS['phenotype_map_overlay'])
def plotPhenotypeMap(infile, outfile):
    '''
    Plot an overlay of a phenotype by geographical distribution
    onto a map of the UK
    '''

    job_memory = "4G"

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2plot.py
    --plot-type=map
    -x %(phenotype_map_overlay)s
    --coordinate-file=%(phenotype_coord_file)s
    --coords-id-col=%(phenotype_id_coords)s
    --lattitude-column=%(phenotype_lat)s
    --longitude-column=%(phenotype_long)s
    --xvar-labels=%(phenotype_xlabels)s
    --reference-value=%(phenotype_ref_value)s
    --var-type=categorical
    --log=%(outfile)s.log
    --output-file=%(outfile)s
    %(infile)s
    '''

    P.run()

# ---------------------------------------------------
# Specific pipeline tasks
@follows(mkdir("plink.dir"),
         dichotimisePhenotype)
@collate("%s/*.%s" % (PARAMS['data_dir'],
                      PARAMS['data_suffix']),
         regex("%s/(.+)(\d+)(.+)\.%s" % (PARAMS['data_dir'],
                                PARAMS['data_suffix'])),
         add_inputs(r"%s/\1\2\3.%s" % (PARAMS['data_dir'],
                                  PARAMS['data_aux'])),
         r"plink.dir/\1\2\3.bed")
def convertToPlink(infiles, outfiles):
    '''
    Convert from other format
    to Plink binary format.  One bed file
    per chromosome - keep the fam files the same
    '''

    job_memory = "60G"
    infiles = ",".join([x for x in infiles[0]])

    log_out = ".".join(outfiles.split(".")[:-1])
    out_pattern = ".".join(outfiles.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=%(data_format)s
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --update-sample-attribute=gender
    --format-parameter=%(format_gender)s
    --method=format
    --memory="60G"
    --format-method=change_format
    --reformat-type=plink_binary
    --output-file-pattern=%(out_pattern)s
    --log=%(log_out)s.log
    %(infiles)s
    '''

    P.run()

# -------------------------------------------------------------------
# some variants have missing ID names - change these with the following
# structure:
# chr_bp_A1_A2
@follows(convertToPlink)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim"]),
           r"plink.dir/\1.exclude")
def nameVariants(infiles, outfile):
    '''
    Some variants have missing file convert these to
    have ID structure: chr_bp_A1_A2 instead - update the
    relevant bim file and generate a list of variants
    to exclude - triallelic, duplicates and overlapping
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    # temporary file name
    temp_file = P.getTempFilename(shared=True)

    job_memory = "2G"

    # use awk on the .bim file to generate replacement IDs

    state0 = '''
    cat %(bim_file)s | awk '{if($2 == ".") {printf("%%s\\t%%s_%%s_%%s_%%s\\t%%s\\t%%s\\t%%s\\t%%s\\n",
    $1,$1,$4,$5,$6,$3,$4,$5,$6)} else{print $0}}' > %(temp_file)s.bim;
    mv %(temp_file)s.bim %(bim_file)s
    '''

    # create files to remove triallelic variants, overlapping variants
    # and duplicates
    state1 = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2geno.py
    --task=detect_duplicates
    --outfile-pattern=%(temp_file)s
    --log=%(outfile)s.log
    %(bim_file)s;
    cat %(temp_file)s.triallelic %(temp_file)s.duplicates
    %(temp_file)s.overlapping | sort | uniq >> %(outfile)s
    '''

    statement = ";".join([state0, state1])
    P.run()

############################
# QC tasks on genotype and #
# samples                  #
############################

# genotype filtering tasks:
# snp genotyping rate
# individual missingness
# individual heterozygosity
# individual relatedness - IBD
# gender check - reported vs genetic gender
# PCA on set of LD pruned SNPs - compare to self-reported ethnicity
# perform each task independently and create exclusion lists. Remove
# all individuals and SNPs at the end.

# GRM will give relatedness and inbreeding at the same time

# First step is to produce a complete LD pruned list of SNPs
# this is slow for lots of SNPs and samples!
@follows(mkdir("QC.dir"),
         nameVariants)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude"]),
           r"QC.dir/\1_round1.prune.in")
def ldPruneSNPsRound1(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this needs to be performed multiple times to get
    # a small enough set of SNPS ~ 10k-50k

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    # this is causing relatedness to be massively
    # overestimated!!
    # devel version allows --keep and --remove
    # use plinkdev for development version

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "5G"
    job_threads = 12

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plinkdev
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --method=ld_prune
    --keep=%(gwas_keep)s
    --use-kb
    --memory=60G
    --ignore-indels
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(ld_prune_threshold)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mkdir("QC.dir"),
         nameVariants,
         ldPruneSNPsRound1)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude",
                       r"QC.dir/\1_round1.prune.in"]),
           r"QC.dir/\1_round2.prune.in")
def ldPruneSNPsRound2(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this needs to be performed multiple times to get
    # a small enough set of SNPS ~ 10k-50k

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    # this is causing relatedness to be massively
    # overestimated!!
    # devel version allows --keep and --remove
    # use plinkdev for development version

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]
    include_file = infiles[1][3]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "5G"
    job_threads = 12

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plinkdev
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --extract-snps=%(include_file)s
    --method=ld_prune
    --keep=%(gwas_keep)s
    --use-kb
    --memory=60G
    --ignore-indels
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(ld_prune_threshold)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mkdir("QC.dir"),
         nameVariants,
         ldPruneSNPsRound2)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude",
                       r"QC.dir/\1_round2.prune.in"]),
           r"QC.dir/\1.prune.in")
def ldPruneSNPsRound3(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this needs to be performed multiple times to get
    # a small enough set of SNPS ~ 10k-50k

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    # this is causing relatedness to be massively
    # overestimated!!
    # devel version allows --keep and --remove
    # use plinkdev for development version

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]
    include_file = infiles[1][3]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "5G"
    job_threads = 12

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plinkdev
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --extract-snps=%(include_file)s
    --method=ld_prune
    --keep=%(gwas_keep)s
    --use-kb
    --memory=60G
    --ignore-indels
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(ld_prune_threshold)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(ldPruneSNPsRound3,
         mkdir("genome.dir"))
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"QC.dir/\1.prune.in"]),
           r"genome.dir/\1_sparse.bed")
def makeTrimmedData(infiles, outfile):
    '''
    Filter on the LD pruned set of SNPs to
    create smaller, sparser genotype files
    Remove duplicates first
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    snps_file = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "32G"
    job_threads = 1

    tmpfile = P.getTempFilename(shared=True)

    # find duplicates
    state1 = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2geno.py
    --log=%(outfile)s.exclude.log
    --task=detect_duplicates
    --outfile-pattern=%(tmpfile)s
    %(bim_file)s
    '''

    exclude_file = tmpfile + ".exclude"

    state2 = '''
    cat %(tmpfile)s.triallelic %(tmpfile)s.duplicates
    %(tmpfile)s.overlapping | sort | uniq >> %(exclude_file)s
    '''

    state3 = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --format-method=change_format
    --reformat-type=plink_binary
    --extract-snps=%(snps_file)s
    --exclude-snps=%(exclude_file)s
    --memory=%(job_memory)s
    --log=%(outfile)s.log
    --output-file-pattern=%(outpattern)s
    --threads=%(job_threads)i
    %(plink_files)s
    '''

    statement = ";".join([state1, state2, state3])

    P.run()

@follows(makeTrimmedData)
@collate(makeTrimmedData,
         regex("genome.dir/(.+)_sparse.bed"),
         add_inputs([r"genome.dir/\1_sparse.fam",
                     r"genome.dir/\1_sparse.bim"]),
         r"genome.dir/WholeGenome.bed")
def mergePlinkFiles(infiles, outfile):
    '''
    Merge all of the LD pruned files together
    to form a set of LD-independent genome-wide
    genotyping files for downstream sample QC
    '''

    # generate text file that contains the names of the files
    # to be merged
    temp_file = P.getTempFilename(shared=True)

    # not all multi-allelic SNPs have been removed properly
    # include the *.missnp file as an exclusion
    
    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "64G"
    job_threads = 1
    with open(temp_file, "w") as ofile:
        for ifile in infiles:
            ifile_bed = ifile[0]
            ifile_fam = ifile[1][0]
            ifile_bim = ifile[1][1]
            ofile.write("%s\t%s\t%s\n" % (ifile_bed, ifile_bim, ifile_fam))

    statement = '''
    plink2
    --merge-list %(temp_file)s
    --threads %(job_threads)i
    --out %(outpattern)s;
    rm -rf %(temp_file)s
    '''

    P.run()


@follows(mergePlinkFiles)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.het.gz")
def calcInbreeding(infiles, outfile):
    '''
    Detect individuals with excess of heterozygosity - indicative
    of population outbreeding/admixture and/or genotyping
    errors
    Also relevant to inbreeding, i.e. depletion of heterozygosity
    indicates individuals more related to themselves than expected
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    temp_file = P.getTempFilename(shared=True)

    job_memory = "4G"
    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --keep-individuals=%(gwas_keep)s
    --summary-method=inbreeding
    --summary-parameter=gz
    --output-file-pattern=%(temp_file)s
    --log=%(outfile)s.log
    %(plink_files)s;
    zcat %(temp_file)s.het.gz | tr -s ' ' '\\t' |
    sed -E 's/^[[:space:]]|[[:space:]]$//g' | gzip > %(outfile)s
    '''

    P.run()


@follows(calcInbreeding)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.ibc")
def findExcessHomozygotes(infiles, outfile):
    '''
    Use the Plink2/GCTA to calculate inbreeding coefficients
    across all individuals expected, i.e. inbred
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-1])

    job_memory = "4G"
    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --summary-method=inbreeding_coef
    --keep-individuals=%(gwas_keep)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    P.run()

@follows(mkdir("QC.dir"))
@transform("%s/chrX*" % PARAMS['data_dir'],
           regex("%s/(.+).bed" % PARAMS['data_dir']),
           add_inputs([r"%s/\1.fam" % PARAMS['data_dir'],
                       r"%s/\1.bim" % PARAMS['data_dir'],
                       r"%s" % PARAMS['qc_pseudo_autosomal']]),
           r"QC.dir/\1.sexcheck")
def genderChecker(infiles, outfile):
    '''
    Check self-reported gender against X-chromosome
    inferred gender.

    Input data are plink binary format with the XY
    pseudoautosomal region removed.  This is required
    for the Plink1.9 gender checking function to work
    properly.

    Output a list of discordant individuals
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    pseudoautosome = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-1])
    job_memory = "6G"
    job_threads = 1

    tmp_file = P.getTempFilename(shared=True)

    state1 = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=check_gender
    --memory=%(job_memory)s
    --exclude-snps=%(pseudoautosome)s
    --keep-individuals=%(gwas_keep)s
    --output-file-pattern=%(tmp_file)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink2.log
    '''

    # swap all spaces for a single tab and remove
    # leading and trailing spaces
    state2 = '''
    cat %(tmp_file)s.sexcheck | tr -s ' ' '\t' |
    sed 's/^[[:space:]]*//g' | sed 's/*[[:space:]]$//g'
    > %(outfile)s
    '''

    statement = ";".join([state1, state2])

    P.run()


@follows(mergePlinkFiles,
         mkdir("grm.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"grm.dir/\1.grm.N.bin")
def makeGRM(infiles, outfiles):
    '''
    Calculate the realised GRM across all LD trimmed
    variants
    Use parallelisation
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "12G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfiles.split(".")[:-3])

    # why does GCTA keep throwing memory errors??
    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --parallel=%(job_threads)s
    --input-file-format=plink_binary
    --keep-individuals=%(gwas_keep)s
    --memory="120G"
    --method=matrix
    --matrix-compression=bin
    --matrix-form=grm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfiles)s.log
    %(plink_files)s
    > %(outfiles)s.gcta.log
    '''

    P.run()


@follows(mergePlinkFiles)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.genome")
def calculateIdentityByDescent(infiles, outfile):
    '''
    Calculate the pair-wise estimates of IBS/IBD
    between individuals.  This will be used to
    flag related individuals
    '''

    job_memory = "20G"
    job_threads = 12

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plink2
    --input-file-format=plink_binary
    --keep-individuals=%(gwas_keep)s
    --parallel=%(job_threads)s
    --memory="240G"
    --threads=%(job_threads)s
    --method=IBD
    --IBD-parameter=norm
    --output-file-pattern=%(outfile)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(calculateIdentityByDescent,
         mkdir("exclusions.dir"))
@transform(calculateIdentityByDescent,
           regex("QC.dir/(.+).genome.gz"),
           r"exclusions.dir/\1.related")
def filterRelated(infile, outfile):
    '''
    Filter individuals on relatedness, taking one
    of each pair of related individuals above
    and IBD sharing > 0.03125, or 3rd cousins.
    '''

    pass


@follows(makeGRM)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.rel.id")
def excludeRelated(infiles, outfile):
    '''
    Find and exclude related individuals with IBD
    >= a threshold. Recommend IBD <= 3rd cousins
    (IBD >= 0.03125) - this can be parallelised

    Plink2 recommend using the binary genotype
    files as input, not the previously computed
    GRM. See here for details:
    https://www.cog-genomics.org/plink2/distance#rel_cutoff

    # Plink seems to be over doing the trimming of
    # individuals based on --rel-cutoff <- needs
    # further investigation.
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    job_memory = "10G"
    job_threads = 24
    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=remove_relations
    --threshold=%(relationship_cutoff)s
    --memory=240G
    --genotype-rate=0.9
    --threads=%(job_threads)s
    --keep-individuals=%(gwas_keep)s
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(makeGRM,
         excludeRelated,
         mkdir("pca.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           r"pca.dir/\1_naive.pcs")
def runNaivePCA(infiles, outfile):
    '''
    Use flashPCA for large datasets to calculate first
    N principal components on LD trimmed genotypes.
    Run on total sample, before filtering for
    self-reported ethnicity

    flashPCA must be in the PATH variable
    '''

    job_threads = PARAMS['pca_threads']
    job_memory = PARAMS['pca_memory']

    plink_prefix = ".".join(infiles.split(".")[:-1])
    out_pattern = ".".join(outfile.split(".")[:-1])
    eigenvecs = out_pattern + ".eigenvec"
    propvar = out_pattern + ".pve"
    outval = out_pattern + ".eigenval"
    outmeansd = out_pattern + ".meansd"
    loads = out_pattern + ".loadings"

    statement = '''
    flashpca
    --numthreads %(job_threads)s
    --seed %(seed)s
    --bfile %(plink_prefix)s
    --ndim 20
    --stand binom
    --method eigen
    --v
    --outpc %(outfile)s
    --outvec %(eigenvecs)s
    --outload %(loads)s
    --outval %(outval)s
    --outpve %(propvar)s
    --outmeansd %(outmeansd)s
    > %(outfile)s.log
    '''

    P.run()


@follows(makeGRM,
         excludeRelated,
         mkdir("pca.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"pca.dir/\1_filtered.pcs")
def runFilteredPCA(infiles, outfile):
    '''
    Use flashPCA for large datasets to calculate first
    N principal components on LD trimmed genotypes.

    Pre-filter samples for ethnicity.

    flashPCA must be in the PATH variable
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])
    temp_out = P.getTempFilename(shared=True)
    
    statement1 = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plinkdev
    --input-file-format=plink_binary
    --method=format
    --keep-individuals=%(gwas_keep)s
    --format-method=change_format
    --reformat-type=plink_binary
    --output-file-pattern=%(temp_out)s
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    job_threads = PARAMS['pca_threads']
    job_memory = PARAMS['pca_memory']

    out_pattern = ".".join(outfile.split(".")[:-1])
    eigenvecs = out_pattern + ".eigenvec"
    propvar = out_pattern + ".pve"
    outval = out_pattern + ".eigenval"
    outmeansd = out_pattern + ".meansd"
    loads = out_pattern + ".loadings"

    statement2 = '''
    flashpca
    --numthreads %(job_threads)s
    --seed %(seed)s
    --bfile %(temp_out)s
    --ndim 20
    --stand binom
    --method eigen
    --v
    --outpc %(outfile)s
    --outvec %(eigenvecs)s
    --outload %(loads)s
    --outval %(outval)s
    --outpve %(propvar)s
    --outmeansd %(outmeansd)s
    > %(outfile)s.log
    '''

    statement = " ; ".join([statement1,
                            statement2])
    P.run()


@follows(runNaivePCA,
         runFilteredPCA)
@transform([runNaivePCA,
            runFilteredPCA],            
           regex("pca.dir/(.+).pcs"),
           add_inputs([PARAMS['data_phenotypes'],
                       r"genome.dir/\1.fam"]),
           r"plots.dir/\1_PC1vsPC2.png")
def plotPcaResults(infiles, outfile):
    '''
    Generate pairwise plots of the first
    n principal components
    '''

    pcs_file = infiles[0]
    phenotypes = infiles[1][0]
    fam_file = infiles[1][1]

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2plot.py
    --plot-type=pca
    --plot-n-pc=2
    --metadata-file=%(phenotypes)s
    --fam-file=%(fam_file)s
    --group-labels=%(format_ethnicity_var)s
    --log=%(outfile)s.log
    --output-file
    %(pcs_file)s
    '''

    P.run()


###############################################
# Parse QC files to get a list of individuals #
# to exclude from analyses                    #
###############################################


@follows(genderChecker,
         mkdir("exclusions.dir"))
@transform(genderChecker,
           regex("QC.dir/(.+).sexcheck"),
           r"exclusions.dir/gender_exclusion.txt")
def excludeDiscordantGender(infile, outfile):
    '''
    Make a list of individuals with discordant gender
    from X chromsome data vs. self-reported gender
    '''

    job_memory = "2G"
    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/qcs2qc.py
    --task=discordant_gender
    --gender-check-file=%(infile)s
    --plotting-path=%(plots_dir)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(findExcessHomozygotes,
         excludeDiscordantGender)
@transform(findExcessHomozygotes,
           regex("QC.dir/(.+).ibc"),
           r"exclusions.dir/\1.inbred")
def excludeInbred(infile, outfile):
    '''
    Flag up individuals with high inbreeding
    coefficients based on a threshold.

    Most human populations have F < 0.05, but
    this may be greatly affect by Ne.

    Use GCTA's Fhat3 as an unbiased estimator
    of F.
    '''

    job_memory = "2G"
    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/qcs2qc.py
    --task=find_inbreds
    --inbreeding-coef-file=%(infile)s
    --inbreeding-coefficient=Fhat3
    --inbred-cutoff=%(qc_inbreed_threshold)0.3f
    --plotting-path=%(plots_dir)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


@follows(calcInbreeding)
@transform(calcInbreeding,
           regex("QC.dir/(.+).het.gz"),
           r"exclusions.dir/\1.het_exclude")
def findExcessHeterozygotes(infile, outfile):
    '''
    Calculate the heterozygosity rate and flag individuals
    with excess heterozygosity indicative of population
    admixture or genotyping errors/contamination.

    Also flag individuals with high inbreeding coefficient.
    Plot these - if there are multiple clusters - indicates
    additional populations of individuals - see UKBiobank
    documentation for details
    '''

    job_memory = "2G"
    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/qcs2qc.py
    --task=flag_hets
    --heterozygotes-file=%(infile)s
    --plotting-path=%(plots_dir)s
    --log=%(outfile)s.log
    > %(outfile)s
    '''

    P.run()


#################################
# Association testing and other #
# statistical testing tasks     #
#################################


if PARAMS['candidate_region']:
    @follows(convertToPlink,
             mkdir("candidate.dir"))
    @transform(convertToPlink,
               regex("plink.dir/%s(.+).bed" % PARAMS['candidate_chromosome']),
               add_inputs([r"plink.dir/%s\1.fam" % PARAMS['candidate_chromosome'],
                           r"plink.dir/%s\1.bim" % PARAMS['candidate_chromosome'],
                           r"plink.dir/%s\1.exclude" % PARAMS['candidate_chromosome']]),
               r"candidate.dir/%s\1-candidate_region.bed" % PARAMS['candidate_chromosome'])
    def getCandidateRegion(infiles, outfile):
        '''
        Pull out genotyping data on individuals over a
        candidate region for testing.
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        exclude = infiles[1][2]

        region = ",".join(PARAMS['candidate_region'].split(":")[-1].split("-"))
        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
        --program=plink2
        --input-file-format=plink_binary
        --method=format
        --keep-individuals=%(gwas_keep)s
        --restrict-chromosome=%(candidate_chromosome)s
        --snp-range=%(region)s
        --exclude-snps=%(exclude)s
        --format-method=change_format
        --format-parameter=%(format_gender)s
        --update-sample-attribute=gender
        --reformat-type=plink_binary
        --output-file-pattern=%(out_pattern)s
        --log=%(outfile)s.log
        %(plink_files)s
        '''

        P.run()

    # make a GRM from the candidate region for MLM analysis
    # need to remove duplicates first
    

    @follows(getCandidateRegion)
    @transform("candidate.dir/*.bed",
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"grm.dir/\1.grm.N.bin")
    def makeCandidateGRM(infiles, outfiles):
        '''
        Calculate the realised GRM across all candidate region
        variants.   Use parallelisation.
        '''

        job_threads = PARAMS['grm_threads']
        # memory per thread
        job_memory = "12G"

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]

        plink_files = ",".join([bed_file, fam_file, bim_file])
        out_pattern = ".".join(outfiles.split(".")[:-3])

        statement= '''
        python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
        --program=gcta
        --threads=%(job_threads)s
        --input-file-format=plink_binary
        --keep-individuals=%(gwas_keep)s
        --method=matrix
        --matrix-compression=bin
        --matrix-form=grm
        --output-file-pattern=%(out_pattern)s
        --log=%(outfiles)s.log
        %(plink_files)s
        > %(outfiles)s.gcta.log
        '''

        P.run()


    @follows(getCandidateRegion)
    @transform(getCandidateRegion,
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"candidate.dir/\1_assoc.assoc")
    def testCandidateRegion(infiles, outfile):
        '''
        Test the candidate region for association using
        Plink basic association - i.e. not a model-specific
        analysis
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        job_threads = PARAMS['candidate_threads']
        job_memory = PARAMS['candidate_memory']

        statement = '''
        python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
        --program=plink2
        --input-file-format=plink_binary
        --method=association
        --keep-individuals=%(gwas_keep)s
        --association-method=assoc
        --genotype-rate=0.01
        --indiv-missing=0.01
        --hardy-weinberg=0.0001
        --min-allele-frequency=0.001
        --output-file-pattern=%(out_pattern)s
        --threads=%(candidate_threads)s
        --log=%(outfile)s.log
        -v 5
        %(plink_files)s
        '''

        P.run()

    @follows(testCandidateRegion)
    @transform(getCandidateRegion,
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"candidate.dir/\1_conditional.%s" % PARAMS['conditional_model'])
    def conditionalTestRegion(infiles, outfile):
        '''
        Perform association analysis conditional on
        top SNP from previous analysis
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
        --program=plink2
        --input-file-format=plink_binary
        --method=association
        --association-method=logistic
        --keep-individuals=%(gwas_keep)s
        --genotype-rate=0.1
        --indiv-missing=0.1
        --hardy-weinberg=0.0001
        --conditional-snp=rs12924101
        --min-allele-frequency=0.01
        --output-file-pattern=%(out_pattern)s
        --threads=%(pca_threads)s
        --log=%(outfile)s.log
        -v 5
        %(plink_files)s
        '''
        
        P.run()
else:
    pass

@follows(convertToPlink,
         mkdir("gwas.dir"))
@transform("plink.dir/chr*",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim"]),
           r"gwas.dir/\1_assoc.assoc")
def unadjustedAssociation(infiles, outfile):
    '''
    Run an unadjusted association analysis on SNPs MAF >= 1%
    Need to condition on array batch - field 22000
    '''

    job_memory = "5G"
    job_threads = int(PARAMS['gwas_threads'])

    mem = int(job_memory.strip("G")) * job_threads
    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=association
    --keep=%(gwas_keep)s
    --association-method=assoc
    --genotype-rate=0.01
    --indiv-missing=0.01
    --hardy-weinberg=0.000001
    --min-allele-frequency=0.001
    --output-file-pattern=%(out_pattern)s
    --threads=%(job_threads)s
    --memory=%(mem)s
    -v 5
    %(plink_files)s
    > %(outfile)s.log
    '''

    P.run()


@follows(excludeDiscordantGender,
         findExcessHeterozygotes,
         findExcessHomozygotes,
         excludeInbred)
@transform("QC.dir/*.het_exclude",
           regex("QC.dir/(.+).het_exclude"),
           add_inputs([r"QC.dir/\1.ibc",
                       r"QC.dir/\1.het_exclude",
                       genderChecker]),
           r"QC.dir/\1.gwas_exclude")
def mergeExclusions(infiles, outfile):
    '''
    Merge together files and drop duplicates
    for individuals that fail QC steps
    '''

    pass


@follows(runFilteredPCA,
         mergeExclusions,
         unadjustedAssociation)
@transform(runFilteredPCA,
           regex("pca.dir/(.+)_filtered.pcs"),
           add_inputs(PARAMS['data_phenotypes']),
           r"phenotypes.dir/\1.covars")
def mergeCovariates(infiles, outfile):
    '''
    Merge together all covariates for inclusion
    in the adjusted GWA
    '''

    pass


@follows(convertToPlink,
         mergeCovariates,
         mergeExclusions,
         mkdir("gwas.dir"))
@transform("plink.dir/chr*",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim"]),
           r"gwas.dir/\1_assoc.assoc")
def pcAdjustedAssociation(infiles, outfile):
    '''
    Run an association analysis on SNPs MAF >= 1%
    adjusted for principal components
    Need to condition on array batch - field 22000
    '''

    job_memory = "5G"
    job_threads = int(PARAMS['gwas_threads'])

    mem = int(job_memory.strip("G")) * job_threads
    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=association
    --keep=%(gwas_keep)s
    --association-method=assoc
    --genotype-rate=0.01
    --indiv-missing=0.01
    --hardy-weinberg=0.000001
    --min-allele-frequency=0.001
    --output-file-pattern=%(out_pattern)s
    --threads=%(job_threads)s
    --memory=%(mem)s
    -v 5
    %(plink_files)s
    > %(outfile)s.log
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation)
@transform(unadjustedAssociation,
           regex("gwas.dir/(.+)_assoc.assoc"),
           r"plots.dir/\1_manhattan.png")
def plotChromosomeManhattan(infile, outfile):
    '''
    Generate a manhattan plot for each chromosome
    from the unadjusted analysis
    '''

    job_memory = "1G"

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/assoc2plot.py
    --plot-type=manhattan
    --resolution=chromosome
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(infile)s
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation,
         pcAdjustedAssociation)
@collate([unadjustedAssociation,
          pcAdjustedAssociation],
         regex("gwas.dir/(.+)_assoc.assoc"),
         r"plots.dir/WholeGenome_manhattan.png")
def plotGenomeManhattan(infiles, outfile):
    '''
    Generate a manhattan plot across each chromosome
    from the unadjusted analysis
    '''

    job_memory = "6G"

    res_files = ",".join(infiles)

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/assoc2plot.py
    --plot-type=manhattan
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    '''

    P.run()

# testing conditional analysis of multiple regions
# make a load of dummy files first
@follows(unadjustedAssociation,
         mkdir("conditional.dir"))
@transform(PARAMS['gwas_hit_regions'],
           regex("(.+)/(.+)-(.+).tsv"),
           r"conditional.dir/\1.tsv")
def splitRegionsFile(infile, outfile):
    '''
    Split a file containing genome intervals
    to pull out for conditional analyses
    '''

    # I LOVE AWK!!!!

    statement = '''
    cat %(infile)s | awk '{if(NR > 1) {printf("conditional.dir/%%i:%%i-%%i_%%s.tsv\\n",
    $1, $2, $3, $4)}}' | awk '{system("touch " $0)}'
    '''

    P.run()


@follows(splitRegionsFile)
@transform("conditional.dir/*.tsv",
           regex("conditional.dir/(.+):(.+)-(.+)_(.+).tsv"),
           add_inputs([r"plink.dir/chr\1impv1.bed",
                       r"plink.dir/chr\1impv1.bim",
                       r"plink.dir/chr\1impv1.fam"]),
           r"conditional.dir/\1:\2-\3_\4.bed")
def getConditionalRegions(infiles, outfile):
    '''
    Pull out the regions of interest for downstream
    conditional analyses
    '''

    conditional_file = infiles[0]
    bed_file = infiles[1][0]
    bim_file = infiles[1][1]
    fam_file = infiles[1][2]

    chrom = conditional_file.split("/")[-1].split(":")[0]
    start = conditional_file.split("/")[-1].split(":")[1].split("-")[0]
    end = conditional_file.split("/")[-1].split(":")[1].split("-")[1]
    end = end.split("_")[0]

    snp_range = ",".join([start, end])
    plink_files = ",".join([bed_file, bim_file, fam_file])

    out_pattern = outfile.strip(".bed")

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py 
    --program=plink2 
    --input-file-format=plink_binary  
    --method=format 
    --restrict-chromosome=%(chrom)s  
    --snp-range=%(snp_range)s
    --format-method=change_format
    --reformat-type=plink_binary
    --keep=%(gwas_keep)s
    --output-file-pattern=%(out_pattern)s
    --memory=30G
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    P.run()


@follows(getConditionalRegions)
@transform(getConditionalRegions,
           regex("candidate.dir/(.+).bed"),
           add_inputs([r"candidate.dir/\1.fam",
                       r"candidate.dir/\1.bim"]),
           r"candidate.dir/\1_conditional.%s" % PARAMS['conditional_model'])
def conditionalAssociation(infiles, outfile):
    '''
    Perform association analysis conditional on
    top SNP from previous analysis
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=association
    --association-method=logistic
    --keep-individuals=%(gwas_keep)s
    --genotype-rate=0.1
    --indiv-missing=0.1
    --hardy-weinberg=0.0001
    --conditional-snp=rs12924101
    --min-allele-frequency=0.01
    --output-file-pattern=%(out_pattern)s
    --threads=%(pca_threads)s
    --log=%(outfile)s.log
    -v 5
    %(plink_files)s
    '''

    P.run()


################################################
# these next tasks aren't strictly GWA, but     #
# they do use genome-wide genotying nonetheless #
#################################################

@follows(mergePlinkFiles,
         mkdir("fst.dir"))
@transform(mergePlinkFiles,
           regex("genome.dir/*.bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"fst.dir/\1.fst")
def getGenomeWideFst(infiles, outfile):
    '''
    Calculate Wright's F-statistic for population
    differentiation, Fst, for all SNPs.

    Use approx. independent SNP set following
    LD pruning.
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --summary-method=wrights_fst
    --summary-parameter="case-control"
    --output-file-pattern%(out_pattern)s
    --log=%(outfile)s.log
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mergePlinkFiles,
         getGenomeWideFst,
         mkdir("fst.dir"))
@transform(mergePlinkFiles,
           regex("plink.dir/*.bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude"]),
           r"fst.dir/\1.fst")
def getFstByChromosome(infiles, outfile):
    '''
    Calculate Wright's F-statistic for population
    differentiation, Fst, for all SNPs.

    Use all SNPs across all chromosomes, minus
    exclusions
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    exclude_file = infiles[1][2]
    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --exclude-snps=%(exclude_file)s
    --summary-method=wrights_fst
    --summary-parameter="case-control"
    --output-file-pattern%(out_pattern)s
    --log=%(outfile)s.log
    -v 5
    %(plink_files)s
    > %(outfile)s.plink.log
    '''

    P.run()


@follows(mkdir("dissect.dir"),
         mkdir("grm.dir"))
@transform("data.dir/subsample_small.bed",
           regex("data.dir/(.+)_small.bed"),
           r"grm.dir/\1.dat")
def test_dissectGrmSingleFile(infiles, outfile):
    '''
    Test DISSECT for parallelised analysis
    Make a GRM, then do PCA
    MUST have a separate phenotypes file with no header, but in plink format
    for GWAS
    '''

    # memory errors with 3G - bump it up to 12G
    # using more processes may also relieve some of the
    # MEMORY ERRORS WITH 12G!!! WTF?? - more processes?? > 300?
    # memory requirements - try it with 192
    # if memory requirements are too high, jobs fail
    # increase the number of processes to spread the job out
    job_memory = "8G"
    infiles = ",".join(infiles)
    job_threads = 128 
    # job_queue = "mpi.q"

    job_queue = ",".join(["all.q@cgat001", "all.q@cgat002", "all.q@cgat003", "all.q@cgat004",
                          "all.q@cgat005", "all.q@cgat006", "all.q@cgat007", "all.q@cgat008",
                          "all.q@cgat009", "all.q@cgat010", "all.q@cgat011", "all.q@cgat012",
                          "all.q@cgat013", "all.q@cgat014", "all.q@cgat101", "all.q@cgat102",
                          "all.q@cgat103", "all.q@cgat104", "all.q@cgat105", "all.q@cgat106",
                          "all.q@cgat107", "all.q@cgat108", "all.q@cgat109", "all.q@cgat110",
                          "all.q@cgat111", "all.q@cgat112", "all.q@cgat113", "all.q@cgat114",
                          "all.q@cgat115", "all.q@cgat116", "all.q@cgatgpu1", "all.q@cgatsmp1"])

    cluster_parallel_environment = " mpi "
    statement = '''
    mpirun
    dissect
    --bfile data.dir/subsample
    --make-grm
    --out grm.dir/subsample
    > dissect_test.log
    '''

    P.run()

@follows(mkdir("dissect.dir"),
         mkdir("grm.dir"))
@transform("data.dir/subsample_list.tsv",
           regex("data.dir/(.+)_list.tsv"),
           r"grm.dir/\1.dat")
def test_dissectGrmManyFiles(infile, outfile):
    '''
    Test DISSECT for parallelised analysis
    Make a GRM, then do PCA
    MUST have a separate phenotypes file with no header, but in plink format
    for GWAS
    '''

    # get the sample list from PARAMS
    # for chr10-9 + chr1, estimated memory usage was 4GB per process
    # failing on 4G memory per node
    job_memory = "3G"
    job_threads = 300
    job_queue = "mpi.q"
    cluster_parallel_environment = " mpi "

    out_pattern = ".".join(outfile.split(".")[:-1])
    statement = '''
    mpirun  --np 1
    dissect
    --bfile-list %(infile)s
    --make-grm
    --grm-join-method 0
    --out %(out_pattern)s
    > dissect_test.log
    '''

    P.run()

@follows(test_dissectGrmManyFiles,
         mkdir("pca.dir"))
@transform("grm.dir/*.dat",
           regex("grm.dir/(.+).dat"),
           r"pca.dir/\1.pca.eigenvalues")
def dissectPCA(infile, outfile):
    '''
    Test DISSECT PCA on grms
    '''
    job_memory = "2G"
    infiles = ",".join(infiles)
    job_threads = 128
    job_options = "-l h=!andromeda,h=!gandalf,h=!saruman"
    job_queue = "all.q"
    cluster_parallel_environment = " mpi "
    statement = '''
    mpirun
    dissect
    --pca
    --grm grm.dir/wholegenome
    --out pca.dir/wholegenome
    --num-eval 20
    > dissect_test.log
    '''
    
    P.run()

# ---------------------------------------------------
# Generic pipeline tasks
@follows(runFilteredPCA,
         plotPcaResults,
         excludeInbred,
         calculateIdentityByDescent,
         mergeExclusions)
def QC():
    pass

@follows(QC,
         mergeCovariates,
         pcAdjustedAssociation,
         plotGenomeManhattan)
def GWAS():
    pass

@follows(QC,
         GWAS,
         getConditionalRegions,
         conditionalAssociation)
def Conditional():
    pass

@follows(Conditional,
         dissectPCA)
def ParallelMLM():
    pass

@follows(QC)
def Evo():
    pass

@follows(QC,
         GWAS,
         Conditional,
         ParallelMLM,
         Evo)
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
