##############################################################################
#
#   MRC FGU CGAT
#
#   $Id$
#
#   Copyright (C) 2009 Mike Morgan
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
###############################################################################
"""===========================
Pipeline GWAS
===========================

:Author: Mike Morgan
:Release: $Id$
:Date: |today|
:Tags: Python

.. Replace the documentation below with your own description of the
   pipeline's purpose

Overview
========

This pipeline computes the word frequencies in the configuration
files :file:``pipeline.ini` and :file:`conf.py`.

Usage
=====

See :ref:`PipelineSettingUp` and :ref:`PipelineRunning` on general
information how to use CGAT pipelines.

Configuration
-------------

The pipeline requires a configured :file:`pipeline.ini` file.
CGATReport report requires a :file:`conf.py` and optionally a
:file:`cgatreport.ini` file (see :ref:`PipelineReporting`).

Default configuration files can be generated by executing:

   python <srcdir>/pipeline_gwas.py config

Input files
-----------

None required except the pipeline configuration files.

Requirements
------------

The pipeline requires the results from
:doc:`pipeline_annotations`. Set the configuration variable
:py:data:`annotations_database` and :py:data:`annotations_dir`.

On top of the default CGAT setup, the pipeline requires the following
software to be in the path:

.. Add any additional external requirements such as 3rd party software
   or R modules below:

Requirements:

* samtools >= 1.1

Pipeline output
===============

.. Describe output files of the pipeline here

Glossary
========

.. glossary::


Code
====

"""
from ruffus import *

import sys
import os
import sqlite3
import CGAT.Experiment as E
import CGATPipelines.Pipeline as P
import PipelineGWAS as gwas

# load options from the config file
PARAMS = P.getParameters(
    ["%s/pipeline.ini" % os.path.splitext(__file__)[0],
     "../pipeline.ini",
     "pipeline.ini"])

# add configuration values from associated pipelines
#
# 1. pipeline_annotations: any parameters will be added with the
#    prefix "annotations_". The interface will be updated with
#    "annotations_dir" to point to the absolute path names.
PARAMS.update(P.peekParameters(
    PARAMS["annotations_dir"],
    "pipeline_annotations.py",
    on_error_raise=__name__ == "__main__",
    prefix="annotations_",
    update_interface=True))


# if necessary, update the PARAMS dictionary in any modules file.
# e.g.:
#
# import CGATPipelines.PipelineGeneset as PipelineGeneset
# PipelineGeneset.PARAMS = PARAMS
#
# Note that this is a hack and deprecated, better pass all
# parameters that are needed by a function explicitely.

# -----------------------------------------------
# Utility functions
def connect():
    '''utility function to connect to database.

    Use this method to connect to the pipeline database.
    Additional databases can be attached here as well.

    Returns an sqlite3 database handle.
    '''

    dbh = sqlite3.connect(PARAMS["database"])
    statement = '''ATTACH DATABASE '%s' as annotations''' % (
        PARAMS["annotations_database"])
    cc = dbh.cursor()
    cc.execute(statement)
    cc.close()

    return dbh

# ---------------------------------------------------
# load the UKBiobank phenotype data into an SQLite DB
# use this as the main accessor of phenotype data
# for the report and non-genetic analyses
@follows(mkdir("phenotypes.dir"))
@transform("%s/*.tab" % PARAMS['data_dir'],
           regex("%s/(.+).tab" % PARAMS['data_dir']),
           r"phenotypes.dir/\1.tsv")
def formatPhenotypeData(infiles, outfile):
    '''
    Use the UKBiobank encoding dictionary/R script to
    set the factor levels for phenotype data.
    Output is in plink covariate file format
    '''

    pheno_file = infiles

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2pheno.py
    --task=plink_format
    --id-variable=%(data_id_var)s
    --log=%(outfile)s.log
    %(pheno_file)s
    > %(outfile)s
    '''

    P.run()

@follows(formatPhenotypeData)
@transform(formatPhenotypeData,
           suffix(".tsv"),
           ".load")
def loadPhenotypes(infile, outfile):
    '''
    load all phenotype data in to an SQLite DB
    '''

    P.load(infile, outfile)


@follows(loadPhenotypes)
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"phenotypes.dir/\1_British.tsv")
def selectBritish(infile, outfile):
    '''
    Select only those individuals with a white British
    ethnicity
    '''

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2pheno.py
    --task=select_ethnicity
    --ethnicity-id=%(format_ethnicity_var)s
    --ethnicity-label=%(format_ethnicity)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(loadPhenotypes,
         selectBritish)
@transform(selectBritish,
           regex("phenotypes.dir/(.+)_British.tsv"),
           r"phenotypes.dir/\1.pheno")
def dichotimisePhenotype(infile, outfile):
    '''
    Dichotomise a phenotype for association testing
    '''

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2pheno.py
    --task=dichotimise_phenotype
    --pheno-id=%(data_dichot_var)s
    --reference-variable=%(data_reference_value)s
    --missing-var-label=%(data_missing_label)s
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(loadPhenotypes,
         mkdir("plots.dir"))
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"plots.dir/1\_phenotype.png")
def plotPhenotypeData(infile, outfile):
    '''
    Generare plots of phenotype distributions
    for CGATReport document
    '''

    pass


@follows(loadPhenotypes,
         plotPhenotypeData)
@transform(formatPhenotypeData,
           regex("phenotypes.dir/(.+).tsv"),
           r"plots.dir/\1_map_%s.png" % PARAMS['phenotype_map_overlay'])
def plotPhenotypeMap(infile, outfile):
    '''
    Plot an overlay of a phenotype by geographical distribution
    onto a map of the UK
    '''

    job_memory = "4G"

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2plot.py
    --plot-type=map
    -x %(phenotype_map_overlay)s
    --coordinate-file=%(phenotype_coord_file)s
    --coords-id-col=%(phenotype_id_coords)s
    --lattitude-column=%(phenotype_lat)s
    --longitude-column=%(phenotype_long)s
    --xvar-labels=%(phenotype_xlabels)s
    --reference-value=%(phenotype_ref_value)s
    --var-type=categorical
    --log=%(outfile)s.log
    --output-file=%(outfile)s
    %(infile)s
    '''

    P.run()

# ---------------------------------------------------
# Specific pipeline tasks
@follows(mkdir("plink.dir"),
         dichotimisePhenotype)
@collate("%s/*.bgen" % PARAMS['data_dir'],
         regex("%s/(.+)\.(.+)" % PARAMS['data_dir']),
         add_inputs("%s/*.sample" % PARAMS['data_dir']),
         r"plink.dir/\1.bed")
def convertToPlink(infiles, outfiles):
    '''
    Convert from Oxford binary (BGEN) format
    to Plink binary format.  One bed file
    per chromosome - keep the fam files the same
    '''

    job_memory = "60G"
    infiles = ",".join([x for x in infiles[0]])

    log_out = ".".join(outfiles.split(".")[:-1])
    out_pattern = ".".join(outfiles.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=%(data_format)s
    --phenotypes-file=%(data_phenotypes)s
    --pheno=%(format_pheno)s
    --update-sample-attribute=gender
    --format-parameter=%(format_gender)s
    --method=format
    --format-method=change_format
    --reformat-type=plink_binary
    --output-file-pattern=%(out_pattern)s
    --log=%(log_out)s.log
    %(infiles)s
    '''

    P.run()

# -------------------------------------------------------------------
# some variants have missing ID names - change these with the following
# structure:
# chr_bp_A1_A2
@follows(convertToPlink)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim"]),
           r"plink.dir/\1.exclude")
def nameVariants(infiles, outfile):
    '''
    Some variants have missing file names convert these to
    have ID structure: chr_bp_A1_A2 instead - update the
    relevant bim file and generate a list of variants
    to exclude - triallelic, duplicates and overlapping
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    # temporary file name
    temp_file = P.getTempFilename(shared=True)

    job_memory = "2G"

    # use awk on the .bim file to generate replacement IDs

    state0 = '''
    cat %(bim_file)s | awk '{if($2 == ".") {printf("%%s\\t%%s_%%s_%%s_%%s\\t%%s\\t%%s\\t%%s\\t%%s\\n",
    $1,$1,$4,$5,$6,$3,$4,$5,$6)} else{print $0}}' > %(temp_file)s.bim;
    mv %(temp_file)s.bim %(bim_file)s
    '''

    # create files to remove triallelic variants, overlapping variants
    # and duplicates
    state1 = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2geno.py
    --task=detect_duplicates
    --outfile-pattern=%(temp_file)s
    %(bim_file)s;
    cat %(temp_file)s.triallelic %(temp_file)s.duplicates
    %(temp_file)s.overlapping | sort | uniq >> %(outfile)s
    '''

    statement = ";".join([state0, state1])
    P.run()


# genotype filtering tasks:
# snp genotyping rate
# individual missingness
# individual heterozygosity
# individual relatedness - IBD
# gender check - reported vs genetic gender
# PCA on set of LD pruned SNPs - compare to self-reported ethnicity
# perform each task independently and create exclusion lists. Remove
# all individuals and SNPs at the end.

# GRM will give relatedness and inbreeding at the same time

# First step is to produce a complete LD pruned list of SNPs
# this is slow for lots of SNPs and samples!
@follows(mkdir("QC.dir"),
         nameVariants)
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"plink.dir/\1.exclude"]),
           r"QC.dir/\1.prune.in")
def ldPruneSNPs(infiles, outfile):
    '''
    LD prune SNPs to create a list of independent SNPs
    genome-wide.  To be used for PCA, GRM, inbreeding,
    and heterozygosity estimation
    '''

    # this selects entirely INDELS if not MAF cut-off
    # is used.  Evidently LD is low between indels,
    # but not between SNPs and indels.  How many
    # INDELs are on the Affy array, and how many
    # are imputed?   What is their imputation accuracy?
    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    exclude_file = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfile.split(".")[:-2])
    job_memory = "5G"
    job_threads = 12

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plink2
    --input-file-format=plink_binary
    --exclude-snps=%(exclude_file)s
    --method=ld_prune
    --use-kb
    --min-allele-frequency=0.01
    --ignore-indels
    --prune-method=%(ld_prune_method)s
    --step-size=%(ld_prune_step)s
    --window-size=%(ld_prune_window)s
    --threshold=%(ld_prune_threshold)s
    --threads=%(job_threads)i
    --output-file-pattern=%(out_pattern)s
    --log=%(outfile)s.log
    %(plink_files)s
    '''

    P.run()


@follows(ldPruneSNPs,
         mkdir("genome.dir"))
@transform(convertToPlink,
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim",
                       r"QC.dir/\1.prune.in"]),
           r"genome.dir/\1_sparse.bed")
def makeTrimmedData(infiles, outfile):
    '''
    Filter on the LD pruned set of SNPs to
    create smaller, sparser genotype files
    Remove duplicates first
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]
    snps_file = infiles[1][2]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "32G"
    job_threads = 1

    tmpfile = P.getTempFilename(shared=True)

    # find duplicates
    state1 = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2geno.py
    --log=%(outfile)s.exclude.log
    --task=detect_duplicates
    --outfile-pattern=%(tmpfile)s
    %(bim_file)s
    '''

    exclude_file = tmpfile + ".exclude"

    state2 = '''
    cat %(tmpfile)s.triallelic %(tmpfile)s.duplicates
    %(tmpfile)s.overlapping | sort | uniq >> %(exclude_file)s
    '''

    state3 = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=format
    --format-method=change_format
    --reformat-type=plink_binary
    --extract-snps=%(snps_file)s
    --exclude-snps=%(exclude_file)s
    --log=%(outfile)s.log
    --output-file-pattern=%(outpattern)s
    --threads=%(job_threads)i
    %(plink_files)s
    '''

    statement = ";".join([state1, state2, state3])

    P.run()

@follows(makeTrimmedData)
@collate(makeTrimmedData,
         regex("genome.dir/(.+)_sparse.bed"),
         add_inputs([r"genome.dir/\1_sparse.fam",
                     r"genome.dir/\1_sparse.bim"]),
         r"genome.dir/WholeGenome.bed")
def mergePlinkFiles(infiles, outfile):
    '''
    Merge all of the LD pruned files together
    to form a set of LD-independent genome-wide
    genotyping files for downstream sample QC
    '''

    # generate text file that contains the names of the files
    # to be merged
    temp_file = P.getTempFilename(shared=True)

    # not all multi-allelic SNPs have been removed properly
    # include the *.missnp file as an exclusion
    
    outpattern = ".".join(outfile.split(".")[:-1])
    job_memory = "64G"
    job_threads = 1
    with open(temp_file, "w") as ofile:
        for ifile in infiles:
            ifile_bed = ifile[0]
            ifile_fam = ifile[1][0]
            ifile_bim = ifile[1][1]
            ofile.write("%s\t%s\t%s\n" % (ifile_bed, ifile_bim, ifile_fam))

    statement = '''
    plink2
    --merge-list %(temp_file)s
    --threads %(job_threads)i
    --out %(outpattern)s;
    rm -rf %(temp_file)s
    '''

    P.run()


@follows(mergePlinkFiles)
@transform(mergePlinkFiles,
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"QC.dir/\1.het.gz")
def calcInbreeding(infiles, outfile):
    '''
    Detect individuals with excess of heterozygosity - indicative
    of population outbreeding/admixture and/or genotyping
    errors
    Also relevant to inbreeding, i.e. depletion of heterozygosity
    indicates individuals more related to themselves than expected
    '''

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    temp_file = P.getTempFilename(shared=True)

    job_memory = "4G"
    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2qc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=summary
    --summary-method=inbreeding
    --summary-parameter=gz
    --output-file-pattern=%(temp_file)s
    --log=%(outfile)s.log
    %(plink_files)s;
    zcat %(temp_file)s.het.gz | tr -s ' ' '\\t' |
    sed -E 's/^[[:space:]]|[[:space:]]$//g' | gzip > %(outfile)s
    '''

    P.run()


@follows(calcInbreeding)
@transform(calcInbreeding,
           regex("QC.dir/(.+).het.gz"),
           r"QC.dir/\1.het_exclude")
def findExcessHeterozygotes(infile, outfile):
    '''
    Calculate the heterozygosity rate and flag individuals
    with excess heterozygosity indicative of population
    admixture or genotyping errors/contamination.

    Also flag individuals with high inbreeding coefficient.
    Plot these - if there are multiple clusters - indicates
    additional populations of individuals - see UKBiobank
    documentation for details
    '''

    job_memory = "2G"
    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/pheno2pheno.py
    --task=flag_hets
    --log=%(outfile)s.log
    %(infile)s
    > %(outfile)s
    '''

    P.run()


@follows(mergePlinkFiles,
         mkdir("grm.dir"))
@transform("genome.dir/WholeGenome.*",
           regex("genome.dir/(.+).bed"),
           add_inputs([r"genome.dir/\1.fam",
                       r"genome.dir/\1.bim"]),
           r"grm.dir/\1.grm.N.bin")
def makeGRM(infiles, outfiles):
    '''
    Calculate the realised GRM across all LD trimmed
    variants
    Use parallelisation
    '''

    job_threads = PARAMS['grm_threads']
    # memory per thread
    job_memory = "12G"

    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])
    out_pattern = ".".join(outfiles.split(".")[:-3])

    # why does GCTA keep throwing memory errors??
    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --parallel=%(job_threads)s
    --input-file-format=plink_binary
    --memory="120G"
    --method=matrix
    --matrix-compression=bin
    --matrix-form=grm
    --output-file-pattern=%(out_pattern)s
    --log=%(outfiles)s.log
    %(plink_files)s
    > %(outfiles)s.gcta.log
    '''

    P.run()

@follows(makeGRM,
         mkdir("pca.dir"))
@transform("grm.dir/*.N.bin",
           regex("grm.dir/(.+).grm.N.bin"),
           add_inputs([r"grm.dir/\1.grm.bin",
                       r"grm.dir/\1.grm.id"]),
           r"pca.dir/\1.eigenvec")
def runPCA(infiles, outfile):
    '''
    Run PCA on GRM(s) - this is too slow to execute on all 150K
    individuals at once.
    '''

    job_threads = PARAMS['pca_threads']
    n_file = infiles[0]
    bin_file = infiles[1][0]
    id_file = infiles[1][1]
    job_memory = "11G"
    grm_files = ",".join([n_file, bin_file, id_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=gcta
    --log=%(outfile)s.log
    --input-file-format=GRM_binary
    --method=pca
    --principal-components=%(pca_components)i
    --threads=%(job_threads)s
    --output-file-pattern=%(out_pattern)s
    %(grm_files)s
    '''

    P.run()


if PARAMS['candidate_region']:
    @follows(convertToPlink,
             mkdir("candidate.dir"))
    @transform(convertToPlink,
               regex("plink.dir/%s(.+).bed" % PARAMS['candidate_chromosome']),
               add_inputs([r"plink.dir/%s\1.fam" % PARAMS['candidate_chromosome'],
                           r"plink.dir/%s\1.bim" % PARAMS['candidate_chromosome']]),
               r"candidate.dir/%s\1-candidate_region.bed" % PARAMS['candidate_chromosome'])
    def getCandidateRegion(infiles, outfile):
        '''
        Pull out genotyping data on individuals over a
        candidate region for testing.
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        region = ",".join(PARAMS['candidate_region'].split(":")[-1].split("-"))
        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
        --program=plink2
        --input-file-format=plink_binary
        --method=format
        --restrict-chromosome=%(candidate_chromosome)s
        --snp-range=%(region)s
        --format-method=change_format
        --format-parameter=%(format_gender)s
        --update-sample-attribute=gender
        --reformat-type=plink_binary
        --output-file-pattern=%(out_pattern)s
        --log=%(outfile)s.log
        %(plink_files)s
        '''

        P.run()

    # make a GRM from the candidate region for MLM analysis

    @follows(getCandidateRegion)
    @transform("candidate.dir/*.bed",
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"grm.dir/\1.grm.N.bin")
    def makeCandidateGRM(infiles, outfiles):
        '''
        Calculate the realised GRM across all candidate region
        variants.   Use parallelisation.
        '''

        job_threads = PARAMS['grm_threads']
        # memory per thread
        job_memory = "12G"

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]

        plink_files = ",".join([bed_file, fam_file, bim_file])
        out_pattern = ".".join(outfiles.split(".")[:-3])

        # why does GCTA keep throwing memory errors??
        statement = '''
        python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
        --program=plink2
        --parallel=%(job_threads)s
        --input-file-format=plink_binary
        --memory="120G"
        --method=matrix
        --matrix-compression=bin
        --matrix-form=grm
        --output-file-pattern=%(out_pattern)s
        --log=%(outfiles)s.log
        %(plink_files)s
        > %(outfiles)s.gcta.log
        '''

        P.run()


    @follows(getCandidateRegion)
    @transform(getCandidateRegion,
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"candidate.dir/\1_assoc.assoc")
    def testCandidateRegion(infiles, outfile):
        '''
        Test the candidate region for association using
        Plink basic association - i.e. not a model-specific
        analysis
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        job_threads = PARAMS['candidate_threads']
        job_memory = PARAMS['candidate_memory']

        statement = '''
        python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
        --program=plink2
        --input-file-format=plink_binary
        --method=association
        --association-method=assoc
        --genotype-rate=0.01
        --indiv-missing=0.01
        --hardy-weinberg=0.0001
        --min-allele-frequency=0.001
        --output-file-pattern=%(out_pattern)s
        --threads=%(candidate_threads)s
        --log=%(outfile)s.log
        -v 5
        %(plink_files)s
        '''

        P.run()

    @follows(testCandidateRegion)
    @transform(getCandidateRegion,
               regex("candidate.dir/(.+).bed"),
               add_inputs([r"candidate.dir/\1.fam",
                           r"candidate.dir/\1.bim"]),
               r"candidate.dir/\1_conditional.%s" % PARAMS['conditional_model'])
    def conditionalTestRegion(infiles, outfile):
        '''
        Perform association analysis conditional on
        top SNP from previous analysis
        '''

        bed_file = infiles[0]
        fam_file = infiles[1][0]
        bim_file = infiles[1][1]
        plink_files = ",".join([bed_file, fam_file, bim_file])

        out_pattern = ".".join(outfile.split(".")[:-1])

        statement = '''
        python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
        --program=plink2
        --input-file-format=plink_binary
        --method=association
        --association-method=logistic
        --genotype-rate=0.1
        --indiv-missing=0.1
        --hardy-weinberg=0.0001
        --conditional-snp=rs12924101
        --min-allele-frequency=0.01
        --output-file-pattern=%(out_pattern)s
        --threads=%(pca_threads)s
        --log=%(outfile)s.log
        -v 5
        %(plink_files)s
        '''
        
        P.run()
else:
    pass

@follows(convertToPlink,
         mkdir("gwas.dir"))
@transform("plink.dir/chr*",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim"]),
           r"gwas.dir/\1_assoc.assoc")
def unadjustedAssociation(infiles, outfile):
    '''
    Run an unadjusted association analysis on SNPs MAF >= 1%
    Need to condition on array batch - field 22000
    '''

    job_memory = "5G"
    job_threads = int(PARAMS['gwas_threads'])

    mem = int(job_memory.strip("G")) * job_threads
    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=association
    --keep=%(gwas_keep)s
    --association-method=assoc
    --genotype-rate=0.01
    --indiv-missing=0.01
    --hardy-weinberg=0.000001
    --min-allele-frequency=0.001
    --output-file-pattern=%(out_pattern)s
    --threads=%(job_threads)s
    --memory=%(mem)s
    -v 5
    %(plink_files)s
    > %(outfile)s.log
    '''

    P.run()

@follows(convertToPlink,
         mkdir("gwas.dir"))
@transform("plink.dir/chr*",
           regex("plink.dir/(.+).bed"),
           add_inputs([r"plink.dir/\1.fam",
                       r"plink.dir/\1.bim"]),
           r"gwas.dir/\1_assoc.assoc")
def pcAdjustedAssociation(infiles, outfile):
    '''
    Run an association analysis on SNPs MAF >= 1%
    adjusted for principal components
    Need to condition on array batch - field 22000
    '''

    job_memory = "5G"
    job_threads = int(PARAMS['gwas_threads'])

    mem = int(job_memory.strip("G")) * job_threads
    bed_file = infiles[0]
    fam_file = infiles[1][0]
    bim_file = infiles[1][1]

    plink_files = ",".join([bed_file, fam_file, bim_file])

    out_pattern = ".".join(outfile.split(".")[:-1])

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/geno2assoc.py
    --program=plink2
    --input-file-format=plink_binary
    --method=association
    --keep=%(gwas_keep)s
    --association-method=assoc
    --genotype-rate=0.01
    --indiv-missing=0.01
    --hardy-weinberg=0.000001
    --min-allele-frequency=0.001
    --output-file-pattern=%(out_pattern)s
    --threads=%(job_threads)s
    --memory=%(mem)s
    -v 5
    %(plink_files)s
    > %(outfile)s.log
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation)
@transform(unadjustedAssociation,
           regex("gwas.dir/(.+)_assoc.assoc"),
           r"plots.dir/\1_manhattan.png")
def plotChromosomeManhattan(infile, outfile):
    '''
    Generate a manhattan plot for each chromosome
    from the unadjusted analysis
    '''

    job_memory = "1G"

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/assoc2plot.py
    --plot-type=manhattan
    --resolution=chromosome
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(infile)s
    '''

    P.run()


@follows(mkdir("plots.dir"),
         unadjustedAssociation)
@collate(unadjustedAssociation,
         regex("gwas.dir/(.+)_assoc.assoc"),
         r"plots.dir/WholeGenome_manhattan.png")
def plotGenomeManhattan(infiles, outfile):
    '''
    Generate a manhattan plot across each chromosome
    from the unadjusted analysis
    '''

    job_memory = "4G"

    res_files = ",".join(infiles)

    statement = '''
    python /ifs/devel/projects/proj045/gwas_pipeline/assoc2plot.py
    --plot-type=manhattan
    --resolution=genome_wide
    --save-path=%(outfile)s
    --log=%(outfile)s.log
    %(res_files)s
    '''

    P.run()


@follows(mkdir("dissect.dir"),
         mkdir("grm.dir"))
@transform("data.dir/*sample.bed",
           regex("data.dir/(.+).bed"),
           r"grm.dir/\1.dat")
def test_dissectGrmSingleFile(infiles, outfile):
    '''
    Test DISSECT for parallelised analysis
    Make a GRM, then do PCA
    MUST have a separate phenotypes file with no header, but in plink format
    for GWAS
    '''

    job_memory = "1G"
    infiles = ",".join(infiles)
    job_threads = 128
    job_options = "-l h=!andromeda,h=!gandalf,h=!saruman"
    job_queue = "mpi.q"
    cluster_parallel_environment = " mpi "
    statement = '''
    mpirun
    dissect
    --bfile data.dir/subsample
    --make-grm
    --out grm.dir/subsample
    '''

    P.run()

@follows(mkdir("dissect.dir"),
         mkdir("grm.dir"))
@transform("data.dir/*_list.txt",
           regex("data.dir/(.+)_list.txt"),
           r"grm.dir/\1.dat")
def test_dissectGrmManyFiles(infile, outfile):
    '''
    Test DISSECT for parallelised analysis
    Make a GRM, then do PCA
    MUST have a separate phenotypes file with no header, but in plink format
    for GWAS
    '''

    # get the sample list from PARAMS
    # for chr10-9 + chr1, estimated memory usage was 4GB per process
    job_memory = "3G"
    job_threads = 256
    job_options = "-l h=!andromeda,h=!gandalf,h=!saruman"
    job_queue = "all.q"
    cluster_parallel_environment = " mpi "

    out_pattern = ".".join(outfile.split(".")[:-1])
    statement = '''
    mpirun
    dissect
    --bfile-list %(infile)s
    --make-grm
    --grm-join-method 1
    --out %(out_pattern)s
    > dissect_test.log
    '''

    P.run()

@follows(test_dissectGrmManyFiles,
         mkdir("pca.dir"))
@transform("grm.dir/*.dat",
           regex("grm.dir/(.+).dat"),
           r"pca.dir/\1.pca.eigenvalues")
def dissectPCA(infile, outfile):
    '''
    Test DISSECT PCA on grms
    '''
    job_memory = "2G"
    infiles = ",".join(infiles)
    job_threads = 128
    job_options = "-l h=!andromeda,h=!gandalf,h=!saruman"
    job_queue = "all.q"
    cluster_parallel_environment = " mpi "
    statement = '''
    mpirun
    dissect
    --pca
    --grm grm.dir/wholegenome
    --out pca.dir/wholegenome
    --num-eval 20
    > dissect_test.log
    '''
    
    P.run()

# ---------------------------------------------------
# Generic pipeline tasks
@follows()
def full():
    pass


@follows(mkdir("report"))
def build_report():
    '''build report from scratch.

    Any existing report will be overwritten.
    '''

    E.info("starting report build process from scratch")
    P.run_report(clean=True)


@follows(mkdir("report"))
def update_report():
    '''update report.

    This will update a report with any changes inside the report
    document or code. Note that updates to the data will not cause
    relevant sections to be updated. Use the cgatreport-clean utility
    first.
    '''

    E.info("updating report")
    P.run_report(clean=False)


@follows(update_report)
def publish_report():
    '''publish report in the CGAT downloads directory.'''

    E.info("publishing report")
    P.publish_report()

if __name__ == "__main__":
    sys.exit(P.main(sys.argv))
